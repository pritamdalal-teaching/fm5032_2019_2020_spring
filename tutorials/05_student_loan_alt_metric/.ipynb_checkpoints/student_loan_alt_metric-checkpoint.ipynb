{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Loan Prepayments: Alternative Goodness-of-Fit Metric & Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we consider an alternative goodness-of-fit metrics for our student loan prepayment prediction problem.  These alternative metrics will be used in your student loan exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import pandas as pd\n",
    "##> import numpy as np\n",
    "##> import sklearn\n",
    "##> pd.options.display.max_rows = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read-in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_train = pd.read_csv('student_loan.csv')\n",
    "##> df_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the columns of our data set with the `DataFrame.info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_train.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Our Features and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in memory, we can separate the features and labels in preparation for model fitting.  We begin with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> lst_features = \\\n",
    "##>     ['loan_age','cosign','income_annual', 'upb',              \n",
    "##>     'monthly_payment','fico','origbalance',\n",
    "##>     'mos_to_repay','repay_status','mos_to_balln',]    \n",
    "##> df_X = df_train[lst_features]\n",
    "##> df_X.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next we do the same for the labels.  Note that in our encoding a `1` stands for prepayment, while a `0` stands for non-prepayment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_y = df_train['paid_label']\n",
    "##> df_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Holdout Set with `train_test_split()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In subsequent sections we will require a holdout set to measure the out-of-sample performance of our models, so let's create that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.model_selection import train_test_split\n",
    "##> X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, random_state = 0, test_size = 0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Explore `X_train` and `X_test` and verify that the `test_size` parameter controls the size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Accuracy, Precision, Reall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll review the traditional goodness-of-fit metrics:  accuracy, precision, recall, and F1.  We'll do this in the context of logistic regression.\n",
    "\n",
    "Let's begin by fitting a logistic regression to the entirety of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.linear_model import LogisticRegression\n",
    "##> mdl_logit = LogisticRegression(random_state = 0)\n",
    "##> mdl_logit.fit(df_X, df_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `predict()` method of our model to generate the predictions of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> arr_pred_logit = mdl_logit.predict(df_X)\n",
    "##> arr_pred_logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at various in-sample accuracy measures of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(\"Accuracy:  \", np.round(mdl_logit.score(df_X, df_y), 3))\n",
    "##> print(\"Precision: \", np.round(sklearn.metrics.precision_score(arr_pred_logit, df_y), 3))\n",
    "##> print(\"Recall:    \", np.round(sklearn.metrics.recall_score(arr_pred_logit, df_y), 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Use the built-in function in `sklearn.metrics` to calculate the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, in-sample goodness-of-fit metrics are usually too optimistic about model performance.  Using a holdout test-set is a simple way to get a sense for how the model will perform in the wild.\n",
    "\n",
    "The following code fits a logistic regression model to the training set that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_logit_holdout = LogisticRegression(random_state = 0)\n",
    "##> mdl_logit_holdout.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code that calculated the out-of-sample goodness of fit metrics on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> arr_pred_logit_holdout = mdl_logit_holdout.predict(X_test)\n",
    "##> \n",
    "##> print(\"Accuracy:  \", np.round(mdl_logit_holdout.score(X_test, y_test), 3))\n",
    "##> print(\"Precision: \", np.round(sklearn.metrics.precision_score(arr_pred_logit_holdout, y_test), 3))\n",
    "##> print(\"Recall:    \", np.round(sklearn.metrics.recall_score(arr_pred_logit_holdout, y_test), 3))\n",
    "##> print(\"F1:        \", np.round(sklearn.metrics.f1_score(arr_pred_logit_holdout, y_test), 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balances of Loans that Actually Prepaid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far our all of our goodness-of-fit measures have focused on tallying the accuracy of individual predictions.  However, ABS investors are not interested in which particular loans prepayed, but rather the total UPB that prepayed.\n",
    "\n",
    "The following code calculates the total UPB of the loans that actually prepayed in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_upb_prepay = \\\n",
    "##>     (\n",
    "##>     df_train[['upb', 'paid_label']]\n",
    "##>         .assign(prepay_upb = lambda df: df.upb * df.paid_label)\n",
    "##>         ['prepay_upb'].sum()\n",
    "##>     )\n",
    "##> dbl_upb_prepay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balances of Predicted Prepays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the balance of the loans that our logistic regression model predicts will prepay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_upb_prepay_logit = \\\n",
    "##>     (\n",
    "##>     df_train\n",
    "##>         .assign(pred_logit = mdl_logit.predict(df_X))\n",
    "##>         .assign(prepay_upb_logit = lambda df: df.pred_logit * df.upb)\n",
    "##>         ['prepay_upb_logit'].sum()\n",
    "##>     )\n",
    "##> \n",
    "##> dbl_upb_prepay_logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the logitstic regression UPB prepay predictions are only 4% of what actually occurred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_upb_prepay_logit / dbl_upb_prepay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value of Total Balance of Loan Prepayment (In-Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, most classification algorithms calculate a probability for each class.  The specific prediction is then simply the class with the highest probability.\n",
    "\n",
    "In `sklearn` we can view these probabilities with the `.predict_proba()` method.  Let's do this with `mdl_logit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_logit.predict_proba(df_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the probability of prepayment is in the second column, which we can isolate as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_logit.predict_proba(df_X)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these probabilities, let's calculate an expected value for the total UPB that will be prepaid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_ev_logit = \\\n",
    "##>     (\n",
    "##>     df_train\n",
    "##>         .assign(pred_logit = mdl_logit.predict_proba(df_X)[:,1])\n",
    "##>         .assign(prepay_upb_logit = lambda df: df.pred_logit * df.upb)\n",
    "##>         ['prepay_upb_logit'].sum()\n",
    "##>     )\n",
    "##> \n",
    "##> dbl_ev_logit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the in-sample expected value calculation is almost exactly in-line with the actual prepayments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_ev_logit / dbl_upb_prepay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value of Total Balance of Loan Prepayments (Out-of-Sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, from a UPB standpoint, our model seems to be working quite well.  However, the above calculation was done in-sample.  Let's try an out-of-sample accuracy measure calculation with our holdout set.\n",
    "\n",
    "We begin by fitting a model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_logit_holdout = LogisticRegression(random_state = 0)\n",
    "##> mdl_logit_holdout.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculated the actual prepayments in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_prepay_test = \\\n",
    "##>     (\n",
    "##>     X_test\n",
    "##>         .merge(y_test, left_index=True, right_index = True)\n",
    "##>         .assign(upb_prepay = lambda df: df.upb * df.paid_label)\n",
    "##>         ['upb_prepay'].sum()    \n",
    "##>     )\n",
    "##> dbl_prepay_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code returns the out-of-sample prediction probabilities for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> mdl_logit_holdout.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Calculate the out-of-sample expected value of prepaid UPB for the hold-out test set; also, find it's proportion relative to the actual prepayments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dbl_prepay_holdout = \\\n",
    "##>     (\n",
    "##>     X_test\n",
    "##>         .assign(pred_holdout = mdl_logit_holdout.predict_proba(X_test)[:, 1])\n",
    "##>         .assign(upb_prepay_holdout = lambda df: df.upb * df.pred_holdout)\n",
    "##>         ['upb_prepay_holdout'].sum()\n",
    "##>     )\n",
    "##> \n",
    "##> dbl_prepay_holdout / dbl_prepay_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Precision, Recall, and F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The holdout set methodology can be generalized to $n$-fold cross validation.  The set of goodness-of-fit measures that result from cross-validation are, in aggregate, more robust than a metric calculated on a single holdout test set.  \n",
    "\n",
    "In this final section, we'll see what the code looks like to generate these cross-validation metrics for a decision tree classifier.\n",
    "\n",
    "Let's begin by instantiating a decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from sklearn.tree import DecisionTreeClassifier\n",
    "##> mdl_tree = DecisionTreeClassifier(random_state = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates F1, precision, and recall via cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> dct_cv = sklearn.model_selection.cross_validate(mdl_tree, df_X, df_y, scoring = ['f1', 'precision', 'recall'], cv = 5)\n",
    "##> dct_cv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Challenge:** Calculate the average F1 score in our cross-validation scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goodness of fit metric that will be most useful to us will be the expected value of prepayed balance.  Unfortunately, this does not fit neatly into the `.cross_validate()` method in the previous section.  Thus, in order to use our expected value of prepayed balance metric in a cross-validation context, we will have to write some of the boiler-plate code that `sklearn.model_selection` takes care of for us. We will do this next time.\n",
    "\n",
    "Once that's done, you will have enough tools to work on the Student Loan assignment, which will involve hyperparameter tuning of the various classification models we have used thus far.  We will use two metrics for the basis of selecting optimal hyperparameters:\n",
    "\n",
    "1. 10-fold CV Expected UPB Prepayed\n",
    "2. 10-fold CV F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sklearn User Guides**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\n",
    "**Sklearn API Documentation**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
